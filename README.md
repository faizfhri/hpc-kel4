# ğŸš€ HPC Matrix Operations Benchmark

**Interactive High Performance Computing Benchmark with Streamlit Dashboard**

A comprehensive HPC project demonstrating distributed computing concepts through matrix operations, featuring a modern web interface for real-time performance analysis.

![HPC](https://img.shields.io/badge/HPC-High%20Performance%20Computing-blue)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED?logo=docker)
![MPI](https://img.shields.io/badge/MPI-Parallel%20Computing-green)
![Python](https://img.shields.io/badge/Python-3.8+-yellow?logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-FF4B4B?logo=streamlit)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
  - [Alpine Linux](#alpine-linux-recommended)
  - [Debian/Ubuntu](#debianubuntu)
- [Quick Start](#-quick-start)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Performance Metrics](#-performance-metrics)
- [Technologies](#-technologies)
- [Contributing](#-contributing)
- [Team](#-team)

---

## ğŸ¯ Overview

This project simulates a **multi-node HPC cluster** using Docker containers to benchmark matrix operations. It compares performance between:

- âš¡ **Serial Execution**: Single-threaded baseline
- ğŸ–¥ï¸ **Single-Node Parallel**: Multi-core on one machine
- ğŸŒ **Multi-Node Distributed**: Cluster computing across multiple nodes

### Key Objectives

1. **Performance Analysis**: Measure speedup and efficiency of parallel algorithms
2. **Scalability Testing**: Evaluate performance across different configurations
3. **Interactive Visualization**: Real-time results through web dashboard
4. **Educational Tool**: Demonstrate HPC concepts in an accessible way

---

## âœ¨ Features

### ğŸ¨ Interactive Streamlit Dashboard
- **Live Benchmarking**: Run tests directly from the browser
- **Real-time Visualization**: Interactive charts with Plotly
- **Custom Configurations**: Adjustable matrix sizes and process counts
- **Results Management**: Save, load, and compare benchmark results

### ğŸ”¬ Comprehensive Analysis
- **Speedup Metrics**: Serial vs parallel performance comparison
- **Efficiency Calculations**: Parallel efficiency and scalability analysis
- **Memory Profiling**: Track memory usage across nodes
- **Multiple Algorithms**: Matrix multiplication, LU decomposition, and more

### ğŸ³ Docker-Based Architecture
- **Portability**: Run anywhere with Docker
- **Reproducibility**: Consistent environment across platforms
- **Easy Scaling**: Add or remove nodes effortlessly
- **Isolated Environment**: No conflicts with host system

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Alpine Linux Host                     â”‚
â”‚                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  Streamlit App (Port 8501)               â”‚ â”‚
â”‚   â”‚  - Interactive UI                        â”‚ â”‚
â”‚   â”‚  - Docker Management                     â”‚ â”‚
â”‚   â”‚  - Benchmark Orchestration               â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                              â”‚
â”‚                   â–¼                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚    Docker Network: mpi-net               â”‚ â”‚
â”‚   â”‚                                           â”‚ â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”‚
â”‚   â”‚  â”‚ hpchead  â”‚â—„â”€â”€â”€â”€â–ºâ”‚  node01  â”‚         â”‚ â”‚
â”‚   â”‚  â”‚ (Master) â”‚      â”‚ (Worker) â”‚         â”‚ â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚
â”‚   â”‚       â”‚                                   â”‚ â”‚
â”‚   â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”‚
â”‚   â”‚       â”‚             â”‚  node02  â”‚         â”‚ â”‚
â”‚   â”‚       â”‚             â”‚ (Worker) â”‚         â”‚ â”‚
â”‚   â”‚       â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚
â”‚   â”‚       â”‚                                   â”‚ â”‚
â”‚   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚ â”‚
â”‚   â”‚                     â”‚  node03  â”‚         â”‚ â”‚
â”‚   â”‚                     â”‚ (Worker) â”‚         â”‚ â”‚
â”‚   â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”‚
â”‚   â”‚                                           â”‚ â”‚
â”‚   â”‚      Shared Volume: mpi_home             â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Prerequisites

### System Requirements
- **OS**: Alpine Linux (recommended) or Debian/Ubuntu
- **RAM**: 4GB minimum, 8GB recommended
- **CPU**: Multi-core processor (4+ cores recommended)
- **Storage**: 10GB free space

### Software Dependencies
- Docker (latest version)
- Docker Compose (optional, for easier management)
- Git
- Python 3.8+

---

## ğŸš€ Installation

### Alpine Linux (Recommended)

```bash
# 1. Clone the repository
git clone https://github.com/faizfhri/hpc-kel4.git
cd hpc-kel4

# 2. Run the automated setup script
chmod +x setup_alpine.sh
./setup_alpine.sh

# 3. Activate virtual environment
source venv/bin/activate

# 4. Start the application
streamlit run app.py
```

### Debian/Ubuntu

```bash
# 1. Clone the repository
git clone https://github.com/faizfhri/hpc-kel4.git
cd hpc-kel4

# 2. Run the automated setup script
chmod +x setup_debian.sh
./setup_debian.sh

# 3. Activate virtual environment
source venv/bin/activate

# 4. Start the application
streamlit run app.py
```

### Using Docker Compose (Alternative)

```bash
# Build and start all containers
docker-compose up -d

# View logs
docker-compose logs -f

# Stop cluster
docker-compose down
```

---

## ğŸ® Quick Start

### 1. Access the Dashboard

Open your browser and navigate to:
```
http://localhost:8501
```

For network access from other devices:
```bash
streamlit run app.py --server.address=0.0.0.0 --server.port=8501
```
Then access via: `http://<your-server-ip>:8501`

### 2. Start the Cluster

Navigate to **ğŸ  Overview** page and click **"ğŸš€ Start Cluster"**

### 3. Run Your First Benchmark

1. Go to **âš¡ Run Benchmark** page
2. Select **Matrix Multiplication** algorithm
3. Set matrix size to **500** (for quick test)
4. Choose **"Compare All"** mode
5. Click **"â–¶ï¸ RUN BENCHMARK"**

### 4. Analyze Results

Visit **ğŸ“ˆ Results & Analysis** to view:
- Interactive charts
- Performance metrics
- Speedup calculations
- Efficiency analysis

---

## ğŸ“– Usage

### Running Benchmarks

#### Serial Execution
```python
# From Streamlit UI:
1. Select "Matrix Multiplication"
2. Choose "Serial" mode
3. Set matrix size (e.g., 1000)
4. Click "Run Benchmark"
```

#### Parallel Execution
```python
# Single-Node (4 processes on one machine)
Mode: Single Node
Processes: 4
Matrix Size: 1000

# Multi-Node (distributed across cluster)
Mode: Multi Node
Processes: 4
Matrix Size: 1000
```

#### Comparison Mode
```python
# Automatically runs all three modes:
- Serial
- Single Node (4 processes)
- Multi Node (4 processes)

# Provides direct comparison and speedup metrics
```

### Manual CLI Usage (Advanced)

```bash
# Enter head node
docker exec -it hpchead bash

# Compile serial version
gcc -o serial serial.c

# Run serial benchmark
./serial 1000

# Compile parallel version
mpicc -o matrix matrix.c -lm

# Run parallel (single node)
mpirun -np 4 --host hpchead ./matrix 1000

# Run parallel (multi node)
mpirun -np 4 --host hpchead,node01,node02,node03 ./matrix 1000
```

---

## ğŸ“ Project Structure

```
hpc-kel4/
â”œâ”€â”€ app.py                      # Main Streamlit application
â”œâ”€â”€ pages/                      # Streamlit pages
â”‚   â”œâ”€â”€ 1_ğŸ _Overview.py       # System status and control
â”‚   â”œâ”€â”€ 2_âš¡_Run_Benchmark.py  # Interactive benchmarking
â”‚   â”œâ”€â”€ 3_ğŸ“ˆ_Results.py        # Results visualization
â”‚   â””â”€â”€ 6_ğŸ“š_Documentation.py  # Comprehensive docs
â”œâ”€â”€ utils/                      # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ docker_manager.py      # Docker container management
â”‚   â”œâ”€â”€ benchmark_runner.py    # Benchmark execution
â”‚   â””â”€â”€ visualizer.py          # Chart generation
â”œâ”€â”€ data/                       # Data storage
â”‚   â””â”€â”€ results/               # Benchmark results (JSON)
â”œâ”€â”€ matrix.c                    # Parallel matrix multiplication (MPI)
â”œâ”€â”€ serial.c                    # Serial matrix multiplication
â”œâ”€â”€ benchmark.sh               # Legacy CLI benchmark script
â”œâ”€â”€ Dockerfile                  # MPI node container image
â”œâ”€â”€ docker-compose.yml         # Cluster orchestration
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ setup_alpine.sh            # Alpine Linux setup
â”œâ”€â”€ setup_debian.sh            # Debian/Ubuntu setup
â””â”€â”€ README.md                  # This file
```

---

## ğŸ“Š Performance Metrics

### Speedup
```
Speedup = T_serial / T_parallel
```
**Ideal**: Linear speedup (S = N processors)

### Efficiency
```
Efficiency = Speedup / Number_of_Processors
```
**Good**: E > 0.70 (70%)

### GFLOPS
```
GFLOPS = Operations / (Time Ã— 10â¹)

For NÃ—N matrix multiplication:
Operations = 2 Ã— NÂ³
```

### Example Results

| Matrix Size | Serial Time | Multi-Node Time | Speedup | Efficiency |
|-------------|-------------|-----------------|---------|------------|
| 500Ã—500     | 2.3s        | 0.8s            | 2.87x   | 71.8%      |
| 1000Ã—1000   | 18.5s       | 5.2s            | 3.56x   | 89.0%      |
| 2000Ã—2000   | 148.2s      | 41.3s           | 3.59x   | 89.8%      |

---

## ğŸ› ï¸ Technologies

### Backend
- **C/C++**: High-performance computation
- **OpenMPI**: Message Passing Interface for parallel processing
- **GCC**: GNU Compiler Collection

### Infrastructure
- **Docker**: Containerization platform
- **Docker Compose**: Multi-container orchestration
- **Alpine Linux**: Lightweight container OS
- **Debian**: Stable MPI node OS

### Frontend & Visualization
- **Python 3.8+**: Application logic
- **Streamlit**: Interactive web framework
- **Plotly**: Interactive charting library
- **Pandas**: Data manipulation

### DevOps
- **Git**: Version control
- **Bash**: Automation scripts

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

### Development Setup

```bash
# Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/hpc-kel4.git

# Create a feature branch
git checkout -b feature/your-feature-name

# Make your changes and commit
git commit -m "Add your feature"

# Push and create pull request
git push origin feature/your-feature-name
```

---

## ğŸ‘¥ Team

**Kelompok 4** - High Performance Computing Course 2025

- Project Lead & Backend Development
- Frontend & UI/UX Design
- DevOps & Infrastructure
- Documentation & Testing

---

## ğŸ“„ License

This project is created for educational purposes as part of the High Performance Computing course.

---

## ğŸ™ Acknowledgments

- Course Instructor: [Instructor Name]
- OpenMPI Community
- Streamlit Team
- Docker Community

---

## ğŸ“ Contact

For questions or feedback, please open an issue on GitHub.

---

<div align="center">

**Built with â¤ï¸ for High Performance Computing Education**

â­ Star this repo if you find it helpful!

</div>